{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799b2218-4451-44fb-a7e9-7d755ca4faa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/mingyulu/miniconda3/envs/torch_gpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "from typing import List\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../CATENets/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import catenets.models.torch.pseudo_outcome_nets as pseudo_outcome_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6fa35ff-9d09-4837-8771-c43427dd0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_data(x_train):\n",
    "    \n",
    "    x_normalized_train = (x_train - np.min(x_train, axis=0)) / (np.max(x_train, axis=0) - np.min(x_train, axis=0))\n",
    "\n",
    "    return x_normalized_train\n",
    "\n",
    "\n",
    "def subgroup_ate(\n",
    "    method: str,\n",
    "    features: List[int],\n",
    "    y_true_train: np.ndarray,\n",
    "    y_true_test: np.ndarray,\n",
    "    estimated_ate_test: np.ndarray,\n",
    "    iss_test: np.ndarray\n",
    ") -> None:\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(  \n",
    "        max_depth=3,\n",
    "        reg_lambda=2,\n",
    "        # min_split_loss=2\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(x_train[:, features], y_true_train)\n",
    "\n",
    "    y_pred = xgb_model.predict(x_test[:, features])\n",
    "    y_pred_train = xgb_model.predict(x_train[:, features])\n",
    "    \n",
    "    ate = np.sum(estimated_ate_test[y_pred == 1])/len(estimated_ate_test)\n",
    "    auroc = metrics.roc_auc_score(y_true_test, y_pred)\n",
    "        \n",
    "    print(\"===================\")\n",
    "    print(\"%s - auroc %s\"%(method, auroc))\n",
    "    print(\"%s - ATE %s\"%(method, ate))\n",
    "\n",
    "def feature_idx(\n",
    "    method: str,\n",
    "    cohort: str,\n",
    "    learner: str\n",
    ")-> List[int]:\n",
    "    \n",
    "    if method == \"shap\":\n",
    "        file_path = f\"../results/{cohort}/naive_shap_top_5_features_{learner}.csv\"\n",
    "    elif method == \"ig\":\n",
    "        file_path = f\"../results/{cohort}/integrated_gradients_top_5_features_{learner}.csv\"\n",
    "    elif method == \"shap - 0 \":\n",
    "        file_path = f\"../results/{cohort}/shapley_value_sampling_top_5_features_{learner}.csv\"\n",
    "        \n",
    "    df = pd.read_csv(file_path,keep_default_na=False)\n",
    "    \n",
    "    df_sorted = df.sort_values(\n",
    "        by='count (%)', \n",
    "        ascending=False\n",
    "    )\n",
    "    print(df_sorted[\"feature\"].head(5).tolist())\n",
    "    \n",
    "    indices  = [ x.columns.get_loc(col) for col in df_sorted[\"feature\"].head(5) ]\n",
    "    \n",
    "    for i in indices:\n",
    "        if i > treatment_index:\n",
    "            i -= 1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2a81b7-4889-4869-baf2-34aed32e1648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47139/2959244314.py:4: DtypeWarning: Columns (9,16,45,57,58,59,60,61,62,63,64,71,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_year = pd.read_csv(\"../data/all_year.csv\", index_col=0)\n"
     ]
    }
   ],
   "source": [
    "fluid_cohort = pd.read_pickle(\"../data/trauma_responder.pkl\")\n",
    "\n",
    "\n",
    "all_year = pd.read_csv(\"../data/all_year.csv\", index_col=0)\n",
    "\n",
    "fluid_cohort = pd.merge(fluid_cohort,all_year[['registryid','iss']],on='registryid', how='left')\n",
    "fluid_cohort[\"iss\"] = pd.to_numeric(fluid_cohort[\"iss\"], errors='coerce')\n",
    "\n",
    "#\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='proc')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='ethnicity')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='residencestate')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='toxicologyresults')))]\n",
    "\n",
    "\n",
    "x = fluid_cohort.loc[:, ~fluid_cohort.columns.isin([\"registryid\",\n",
    "                                                            \"COV\",\n",
    "                                                            \"TT\",\n",
    "                                                            \"scenegcsmotor\",\n",
    "                                                            \"scenegcseye\",\n",
    "                                                            \"scenegcsverbal\",\n",
    "                                                            \"edgcsmotor\",\n",
    "                                                            \"edgcseye\",\n",
    "                                                            \"edgcsverbal\",\n",
    "                                                            \"outcome\",\n",
    "                                                            \"sex_F\",\n",
    "                                                            \"traumatype_P\",\n",
    "                                                            \"traumatype_other\"\n",
    "                                                            ])]\n",
    "\n",
    "n, feature_size = x.shape\n",
    "names = x.drop([\"treated\"], axis=1).columns\n",
    "treatment_index = x.columns.get_loc(\"treated\")\n",
    "iss_index = x.columns.get_loc(\"iss\")\n",
    "\n",
    "var_index = [i for i in range(feature_size) if i != treatment_index and i != iss_index]\n",
    "\n",
    "x_norm = normalize_data(x)\n",
    "\n",
    "## impute missing value\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(x_norm)\n",
    "x_train_scaled = imp.transform(x_norm)\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "                                             x_train_scaled,  \n",
    "                                             fluid_cohort[\"outcome\"], \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=42,\n",
    "                                             stratify=fluid_cohort[\"treated\"]\n",
    "                                    )\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "                                             x_train,  \n",
    "                                             y_train, \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=42,\n",
    "                                             stratify=x_train[:, treatment_index]\n",
    "                                    )\n",
    "\n",
    "w_train = x_train[:, treatment_index]\n",
    "w_val = x_val[:, treatment_index]\n",
    "w_test =  x_test[:, treatment_index]\n",
    "\n",
    "\n",
    "iss_train = x_train[:, iss_index]\n",
    "iss_test =  x_test[:, iss_index]\n",
    "\n",
    "x_train = x_train[:,var_index]\n",
    "x_val = x_val[:, var_index]\n",
    "x_test = x_test[:, var_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f081309-d8c0-44de-b41c-dbf3f96c1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12698728051396643 0.019538631050279247\n",
      "0.14159337346752995 0.034385373991550984\n",
      "['sex_M', 'edgcs', 'traumatype_B', 'scenegcs', 'causecode_CUT']\n",
      "['temps2', 'HGB', 'sex_M', 'PH', 'FIO2']\n",
      "['temps2', 'scenefirstpulse', 'age', 'HGB', 'sex_M']\n",
      "mean ISS:  29.377541074909495\n",
      "original 0.027823786372953855\n",
      "original - iss 29.918212981593268 21.7\n",
      "===================================\n",
      "===================\n",
      "shap - auroc 0.6911122751877073\n",
      "shap - ATE 0.11168783061171111\n",
      "===================\n",
      "shap - 0  - auroc 0.6092194866422211\n",
      "shap - 0  - ATE 0.0845466122616352\n",
      "===================\n",
      "ig - auroc 0.5537803387462895\n",
      "ig - ATE 0.06903634075141254\n",
      "===================\n",
      "clinical - auroc 0.535533438100227\n",
      "clinical - ATE 0.08969280714181398\n",
      "===================\n",
      "full features - auroc 0.7322332809498865\n",
      "full features - ATE 0.11218239073450124\n",
      "===================\n",
      "random features - auroc 0.607735288982015\n",
      "random features - ATE 0.11644143819844192\n"
     ]
    }
   ],
   "source": [
    "results_train = pkl.load(open(\"../results/responder/train_xlearner.pkl\", \"rb\"))\n",
    "results_test = pkl.load(open(\"../results/responder/test_xlearner.pkl\", \"rb\"))\n",
    "\n",
    "print(np.mean(results_train), np.std(results_train)/np.sqrt(results_train.shape[1]))\n",
    "print(np.mean(results_test), np.std(results_test)/np.sqrt(results_test.shape[1]))\n",
    "\n",
    "estimated_ate_train = np.mean(results_train, axis=0)\n",
    "estimated_ate_test = np.mean(results_test, axis=0)\n",
    "threshold = np.mean(estimated_ate_train)\n",
    "\n",
    "y_true_train = (estimated_ate_train > threshold)\n",
    "y_true_test = (estimated_ate_test > threshold)\n",
    "\n",
    "scenefirstbloodpressure = x.columns.get_loc(\"scenefirstbloodpressure\")\n",
    "lac  = x.columns.get_loc(\"LAC\")\n",
    "inr  = x.columns.get_loc(\"INR\")\n",
    "hgb  = x.columns.get_loc(\"HGB\")\n",
    "\n",
    "explainers = {\n",
    "    \n",
    "    \"shap\": feature_idx(\n",
    "        \"shap\",   \n",
    "        \"responder\",\n",
    "        \"xlearner\"\n",
    "        ),\n",
    "    \"shap - 0 \": feature_idx(\n",
    "        \"shap - 0 \",\n",
    "        \"responder\",\n",
    "        \"xlearner\"\n",
    "    ), #[temp, ph, bd, hgb, pulse ]\n",
    "    \"ig\": feature_idx(\n",
    "        \"ig\",\n",
    "        \"responder\" ,\n",
    "        \"xlearner\"\n",
    "    ), #[ph, na, temp, gender, fio2 ],\n",
    "    \n",
    "    \"clinical\": [lac, inr, hgb,scenefirstbloodpressure ],\n",
    "    \"full features\": [ i for i in range(x_train.shape[1])],\n",
    "    \"random features\": np.random.randint(x_train.shape[1], size=(5)),\n",
    "}\n",
    "\n",
    "\n",
    "print(\"mean ISS: \", np.mean(iss_test)*74+1)\n",
    "print(\"original\", np.sum(estimated_ate_test[w_test==1])/n)\n",
    "print(\"original - iss\", np.mean(iss_test[w_test==1])*74+1, np.mean(iss_test[w_test==0])*74+1)\n",
    "print(\"===================================\")\n",
    "\n",
    "for explainer, features in explainers.items():\n",
    "    subgroup_ate(\n",
    "        explainer,\n",
    "        features,\n",
    "        y_true_train,\n",
    "        y_true_test,\n",
    "        estimated_ate_test,\n",
    "        iss_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f67a45-2c9a-42a6-b58d-2be34de5f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12698728051396643 0.019538631050279247\n",
      "0.14159337346752995 0.034385373991550984\n",
      "['age', 'sex_M', 'causecode_MC', 'causecode_CUT', 'traumatype_B']\n",
      "['age', 'traumatype_B', 'FIB', 'PH', 'sex_M']\n",
      "['traumatype_B', 'HGB', 'sex_M', 'temps2', 'HCT']\n",
      "mean ISS:  29.377541074909495\n",
      "original 0.027823786372953855\n",
      "original - iss 29.918212981593268 21.7\n",
      "===================================\n",
      "===================\n",
      "shap - auroc 0.6937314475292474\n",
      "shap - ATE 0.08998480277728767\n",
      "===================\n",
      "shap - 0  - auroc 0.6284267504801816\n",
      "shap - 0  - ATE 0.09135533581146246\n",
      "===================\n",
      "ig - auroc 0.6079099004714511\n",
      "ig - ATE 0.08970955345675495\n",
      "===================\n",
      "clinical - auroc 0.535533438100227\n",
      "clinical - ATE 0.08969280714181398\n",
      "===================\n",
      "full features - auroc 0.7322332809498865\n",
      "full features - ATE 0.11218239073450124\n",
      "===================\n",
      "random features - auroc 0.570543041732146\n",
      "random features - ATE 0.08892237886895217\n"
     ]
    }
   ],
   "source": [
    "results_train = pkl.load(open(\"../results/responder/train_xlearner.pkl\", \"rb\"))\n",
    "results_test = pkl.load(open(\"../results/responder/test_xlearner.pkl\", \"rb\"))\n",
    "\n",
    "print(np.mean(results_train), np.std(results_train)/np.sqrt(results_train.shape[1]))\n",
    "print(np.mean(results_test), np.std(results_test)/np.sqrt(results_test.shape[1]))\n",
    "\n",
    "estimated_ate_train = np.mean(results_train, axis=0)\n",
    "estimated_ate_test = np.mean(results_test, axis=0)\n",
    "threshold = np.mean(estimated_ate_train)\n",
    "\n",
    "y_true_train = (estimated_ate_train > threshold)\n",
    "y_true_test = (estimated_ate_test > threshold)\n",
    "\n",
    "scenefirstbloodpressure = x.columns.get_loc(\"scenefirstbloodpressure\")\n",
    "lac  = x.columns.get_loc(\"LAC\")\n",
    "inr  = x.columns.get_loc(\"INR\")\n",
    "hgb  = x.columns.get_loc(\"HGB\")\n",
    "\n",
    "explainers = {\n",
    "    \n",
    "    \"shap\": feature_idx(\n",
    "        \"shap\",   \n",
    "        \"responder\",\n",
    "        \"ensemble\"\n",
    "        ),\n",
    "    \"shap - 0 \": feature_idx(\n",
    "        \"shap - 0 \",\n",
    "        \"responder\",\n",
    "        \"ensemble\"\n",
    "    ), #[temp, ph, bd, hgb, pulse ]\n",
    "    \"ig\": feature_idx(\n",
    "        \"ig\",\n",
    "        \"responder\" ,\n",
    "        \"ensemble\"\n",
    "    ), #[ph, na, temp, gender, fio2 ],\n",
    "    \n",
    "    \"clinical\": [lac, inr, hgb,scenefirstbloodpressure ],\n",
    "    \"full features\": [ i for i in range(x_train.shape[1])],\n",
    "    \"random features\": np.random.randint(x_train.shape[1], size=(5)),\n",
    "}\n",
    "\n",
    "\n",
    "print(\"mean ISS: \", np.mean(iss_test)*74+1)\n",
    "print(\"original\", np.sum(estimated_ate_test[w_test==1])/n)\n",
    "print(\"original - iss\", np.mean(iss_test[w_test==1])*74+1, np.mean(iss_test[w_test==0])*74+1)\n",
    "print(\"===================================\")\n",
    "\n",
    "for explainer, features in explainers.items():\n",
    "    subgroup_ate(\n",
    "        explainer,\n",
    "        features,\n",
    "        y_true_train,\n",
    "        y_true_test,\n",
    "        estimated_ate_test,\n",
    "        iss_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26086275-db00-4b9a-99df-df5e791d80c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
