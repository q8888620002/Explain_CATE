{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e73bdb83-710b-45f9-8b7b-c4169bbbe0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "\n",
    "from typing import List\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../CATENets/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import catenets.models.torch.pseudo_outcome_nets as pseudo_outcome_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f37ae636-11b6-4926-bcc1-7a184a9c601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_data(x_train):\n",
    "    \n",
    "    x_normalized_train = (x_train - np.min(x_train, axis=0)) / (np.max(x_train, axis=0) - np.min(x_train, axis=0))\n",
    "\n",
    "    return x_normalized_train\n",
    "\n",
    "\n",
    "def subgroup_ate(\n",
    "    method: str,\n",
    "    features: List[int],\n",
    "    y_true_train: np.ndarray,\n",
    "    y_true_test: np.ndarray,\n",
    "    estimated_ate_test: np.ndarray,\n",
    "    iss_test: np.ndarray\n",
    ") -> None:\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(  \n",
    "        max_depth=3,\n",
    "        reg_lambda=2,\n",
    "        # min_split_loss=2\n",
    "    )\n",
    "\n",
    "    xgb_model.fit(x_train[:, features], y_true_train)\n",
    "\n",
    "    y_pred = xgb_model.predict(x_test[:, features])\n",
    "    y_pred_train = xgb_model.predict(x_train[:, features])\n",
    "    \n",
    "    ate = np.sum(estimated_ate_test[y_pred == 1])/len(estimated_ate_test)\n",
    "    auroc = metrics.roc_auc_score(y_true_test, y_pred)\n",
    "        \n",
    "    print(\"===================\")\n",
    "    print(\"%s - auroc %s\"%(method, auroc))\n",
    "    print(\"%s - ATE %s\"%(method, ate))\n",
    "\n",
    "def feature_idx(\n",
    "    method: str,\n",
    "    cohort: str,\n",
    "    learner: str\n",
    ")-> List[int]:\n",
    "    \n",
    "    if method == \"shap\":\n",
    "        file_path = f\"../results/{cohort}/naive_shap_top_5_features_{learner}.csv\"\n",
    "    elif method == \"ig\":\n",
    "        file_path = f\"../results/{cohort}/integrated_gradients_top_5_features_{learner}.csv\"\n",
    "    elif method == \"shap - 0 \":\n",
    "        file_path = f\"../results/{cohort}/shapley_value_sampling_top_5_features_{learner}.csv\"\n",
    "        \n",
    "    df = pd.read_csv(file_path,keep_default_na=False)\n",
    "    \n",
    "    df_sorted = df.sort_values(\n",
    "        by='count (%)', \n",
    "        ascending=False\n",
    "    )\n",
    "    print(df_sorted[\"feature\"].head(5).tolist())\n",
    "    \n",
    "    indices  = [ x.columns.get_loc(col) for col in df_sorted[\"feature\"].head(5) ]\n",
    "    \n",
    "    for i in indices:\n",
    "        if i > treatment_index:\n",
    "            i -= 1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f956817-82cf-445c-b6ec-531f09b1083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47241/1845283306.py:4: DtypeWarning: Columns (9,16,45,57,58,59,60,61,62,63,64,71,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_year = pd.read_csv(\"../data/all_year.csv\", index_col=0)\n"
     ]
    }
   ],
   "source": [
    "fluid_cohort = pd.read_pickle(\"../data/low_bp_survival.pkl\")\n",
    "\n",
    "\n",
    "all_year = pd.read_csv(\"../data/all_year.csv\", index_col=0)\n",
    "\n",
    "fluid_cohort = pd.merge(fluid_cohort,all_year[['registryid','iss']],on='registryid', how='left')\n",
    "fluid_cohort[\"iss\"] = pd.to_numeric(fluid_cohort[\"iss\"], errors='coerce')\n",
    "\n",
    "#\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='proc')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='ethnicity')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='residencestate')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='toxicologyresults')))]\n",
    "\n",
    "\n",
    "x = fluid_cohort.loc[:, ~fluid_cohort.columns.isin([\"registryid\",\n",
    "                                                            \"COV\",\n",
    "                                                            \"TT\",\n",
    "                                                            \"scenegcsmotor\",\n",
    "                                                            \"scenegcseye\",\n",
    "                                                            \"scenegcsverbal\",\n",
    "                                                            \"edgcsmotor\",\n",
    "                                                            \"edgcseye\",\n",
    "                                                            \"edgcsverbal\",\n",
    "                                                            \"outcome\",\n",
    "                                                            \"sex_F\",\n",
    "                                                            \"traumatype_P\",\n",
    "                                                            \"traumatype_other\"\n",
    "                                                            ])]\n",
    "\n",
    "n, feature_size = x.shape\n",
    "names = x.drop([\"treated\"], axis=1).columns\n",
    "treatment_index = x.columns.get_loc(\"treated\")\n",
    "iss_index = x.columns.get_loc(\"iss\")\n",
    "\n",
    "var_index = [i for i in range(feature_size) if i != treatment_index and i != iss_index]\n",
    "\n",
    "x_norm = normalize_data(x)\n",
    "\n",
    "## impute missing value\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(x_norm)\n",
    "x_train_scaled = imp.transform(x_norm)\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "                                             x_train_scaled,  \n",
    "                                             fluid_cohort[\"outcome\"], \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=42,\n",
    "                                             stratify=fluid_cohort[\"treated\"]\n",
    "                                    )\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "                                             x_train,  \n",
    "                                             y_train, \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=42,\n",
    "                                             stratify=x_train[:, treatment_index]\n",
    "                                    )\n",
    "\n",
    "w_train = x_train[:, treatment_index]\n",
    "w_val = x_val[:, treatment_index]\n",
    "w_test =  x_test[:, treatment_index]\n",
    "\n",
    "\n",
    "iss_train = x_train[:, iss_index]\n",
    "iss_test =  x_test[:, iss_index]\n",
    "\n",
    "x_train = x_train[:,var_index]\n",
    "x_val = x_val[:, var_index]\n",
    "x_test = x_test[:, var_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6935de-6c96-46c1-9f8b-f19b0611dd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04359583797298134 0.01224235412480223\n",
      "-0.044979371980935406 0.02237256661326439\n",
      "['traumatype_B', 'edgcs', 'causecode_FALL', 'sex_M', 'causecode_CUT']\n",
      "['temps2', 'HCT', 'NA', 'HGB', 'edfirstpulse']\n",
      "['temps2', 'HCT', 'traumatype_B', 'PH', 'NA']\n",
      "mean ISS:  29.1565598580524\n",
      "original -0.007728684288254787\n",
      "original - iss 32.176198801198794 25.356564783531073\n",
      "===================================\n",
      "===================\n",
      "shap - auroc 0.794240317775571\n",
      "shap - ATE 0.039242999966659434\n",
      "===================\n",
      "shap - 0  - auroc 0.5737835153922541\n",
      "shap - 0  - ATE -0.0031156017095202676\n",
      "===================\n",
      "ig - auroc 0.6503475670307846\n",
      "ig - ATE 0.009749850991010594\n",
      "===================\n",
      "clinical - auroc 0.5156901688182721\n",
      "clinical - ATE -0.024012609621491376\n",
      "===================\n",
      "full features - auroc 0.8109235352532274\n",
      "full features - ATE 0.040060226552290336\n",
      "===================\n",
      "random features - auroc 0.5940913604766633\n",
      "random features - ATE 0.00457393794751434\n"
     ]
    }
   ],
   "source": [
    "results_train = pkl.load(open(\"../results/massive_trans/train_xlearner.pkl\", \"rb\"))\n",
    "results_test = pkl.load(open(\"../results/massive_trans/test_xlearner.pkl\", \"rb\"))\n",
    "\n",
    "print(np.mean(results_train), np.std(results_train)/np.sqrt(results_train.shape[1]))\n",
    "print(np.mean(results_test), np.std(results_test)/np.sqrt(results_test.shape[1]))\n",
    "\n",
    "estimated_ate_train = np.mean(results_train, axis=0)\n",
    "estimated_ate_test = np.mean(results_test, axis=0)\n",
    "threshold = np.mean(estimated_ate_train)\n",
    "\n",
    "y_true_train = (estimated_ate_train > threshold)\n",
    "y_true_test = (estimated_ate_test > threshold)\n",
    "\n",
    "scenefirstbloodpressure = x.columns.get_loc(\"scenefirstbloodpressure\")\n",
    "lac  = x.columns.get_loc(\"LAC\")\n",
    "inr  = x.columns.get_loc(\"INR\")\n",
    "hgb  = x.columns.get_loc(\"HGB\")\n",
    "\n",
    "explainers = {\n",
    "    \n",
    "    \"shap\": feature_idx(\"shap\",\"massive_trans\", \"xlearner\"),\n",
    "    \"shap - 0 \": feature_idx(\"shap - 0 \",\"massive_trans\" ,\"xlearner\"), #[temp, ph, bd, hgb, pulse ]\n",
    "    \"ig\": feature_idx(\"ig\",\"massive_trans\" ,\"xlearner\"), #[ph, na, temp, gender, fio2 ],\n",
    "    \n",
    "    \"clinical\": [lac, inr, hgb,scenefirstbloodpressure ],\n",
    "    \"full features\": [ i for i in range(x_train.shape[1])],\n",
    "    \"random features\": np.random.randint(x_train.shape[1], size=(5)),\n",
    "}\n",
    "\n",
    "\n",
    "print(\"mean ISS: \", np.mean(iss_test)*74+1)\n",
    "print(\"original\", np.sum(estimated_ate_test[w_test==1])/n)\n",
    "print(\"original - iss\", np.mean(iss_test[w_test==1])*74+1, np.mean(iss_test[w_test==0])*74+1)\n",
    "print(\"===================================\")\n",
    "\n",
    "for explainer, features in explainers.items():\n",
    "    subgroup_ate(\n",
    "        explainer,\n",
    "        features,\n",
    "        y_true_train,\n",
    "        y_true_test,\n",
    "        estimated_ate_test,\n",
    "        iss_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31ffa85-52cf-4308-bad7-15b5813f6669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04359583797298134 0.01224235412480223\n",
      "-0.044979371980935406 0.02237256661326439\n",
      "['causecode_PEDESTRIAN', 'sex_M', 'edgcs', 'scenegcs', 'traumatype_B']\n",
      "['edgcs', 'scenegcs', 'traumatype_B', 'scenefirstpulse', 'HGB']\n",
      "['FIO2', 'HGB', 'sex_M', 'temps2', 'traumatype_B']\n",
      "mean ISS:  29.1565598580524\n",
      "original -0.007728684288254787\n",
      "original - iss 32.176198801198794 25.356564783531073\n",
      "===================================\n",
      "===================\n",
      "shap - auroc 0.7263654419066534\n",
      "shap - ATE 0.03105440185826958\n",
      "===================\n",
      "shap - 0  - auroc 0.7379841112214497\n",
      "shap - 0  - ATE 0.029720891329612746\n",
      "===================\n",
      "ig - auroc 0.6919066534260179\n",
      "ig - ATE 0.02325804521797679\n",
      "===================\n",
      "clinical - auroc 0.5156901688182721\n",
      "clinical - ATE -0.024012609621491376\n",
      "===================\n",
      "full features - auroc 0.8109235352532274\n",
      "full features - ATE 0.040060226552290336\n",
      "===================\n",
      "random features - auroc 0.5857497517378352\n",
      "random features - ATE 0.0009342930210170483\n"
     ]
    }
   ],
   "source": [
    "results_train = pkl.load(open(\"../results/massive_trans/train_xlearner.pkl\", \"rb\"))\n",
    "results_test = pkl.load(open(\"../results/massive_trans/test_xlearner.pkl\", \"rb\"))\n",
    "\n",
    "print(np.mean(results_train), np.std(results_train)/np.sqrt(results_train.shape[1]))\n",
    "print(np.mean(results_test), np.std(results_test)/np.sqrt(results_test.shape[1]))\n",
    "\n",
    "estimated_ate_train = np.mean(results_train, axis=0)\n",
    "estimated_ate_test = np.mean(results_test, axis=0)\n",
    "threshold = np.mean(estimated_ate_train)\n",
    "\n",
    "y_true_train = (estimated_ate_train > threshold)\n",
    "y_true_test = (estimated_ate_test > threshold)\n",
    "\n",
    "scenefirstbloodpressure = x.columns.get_loc(\"scenefirstbloodpressure\")\n",
    "lac  = x.columns.get_loc(\"LAC\")\n",
    "inr  = x.columns.get_loc(\"INR\")\n",
    "hgb  = x.columns.get_loc(\"HGB\")\n",
    "\n",
    "explainers = {\n",
    "    \n",
    "    \"shap\": feature_idx(\"shap\",\"massive_trans\", \"ensemble\"),\n",
    "    \"shap - 0 \": feature_idx(\"shap - 0 \",\"massive_trans\" ,\"ensemble\"), #[temp, ph, bd, hgb, pulse ]\n",
    "    \"ig\": feature_idx(\"ig\",\"massive_trans\" ,\"ensemble\"), #[ph, na, temp, gender, fio2 ],\n",
    "    \n",
    "    \"clinical\": [lac, inr, hgb,scenefirstbloodpressure ],\n",
    "    \"full features\": [ i for i in range(x_train.shape[1])],\n",
    "    \"random features\": np.random.randint(x_train.shape[1], size=(5)),\n",
    "}\n",
    "\n",
    "\n",
    "print(\"mean ISS: \", np.mean(iss_test)*74+1)\n",
    "print(\"original\", np.sum(estimated_ate_test[w_test==1])/n)\n",
    "print(\"original - iss\", np.mean(iss_test[w_test==1])*74+1, np.mean(iss_test[w_test==0])*74+1)\n",
    "print(\"===================================\")\n",
    "\n",
    "for explainer, features in explainers.items():\n",
    "    subgroup_ate(\n",
    "        explainer,\n",
    "        features,\n",
    "        y_true_train,\n",
    "        y_true_test,\n",
    "        estimated_ate_test,\n",
    "        iss_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b2e47-6f2c-48d1-b823-ac3f76e4b97f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
