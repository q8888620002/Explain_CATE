{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b303b39-fc42-4284-a5ca-355dcaa24bca",
   "metadata": {},
   "source": [
    "## Responder cohort analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "c8c731f6-d7c0-47d9-84a8-59370707eef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40033/2438182252.py:4: DtypeWarning: Columns (9,16,45,57,58,59,60,61,62,63,64,71,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_year = pd.read_csv(\"../data/all_year.csv\", index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[po_estimator_0_impute_pos] Epoch: 0, current validation loss: 0.5364680886268616, train_loss: 0.6333889961242676\n",
      "[po_estimator_0_impute_pos] Epoch: 50, current validation loss: 0.4686484932899475, train_loss: 0.3546101152896881\n",
      "[po_estimator_0_impute_pos] Epoch: 100, current validation loss: 0.41848334670066833, train_loss: 0.21636052429676056\n",
      "[po_estimator_0_impute_pos] Epoch: 150, current validation loss: 0.3715575933456421, train_loss: 0.13549907505512238\n",
      "[po_estimator_0_impute_pos] Epoch: 200, current validation loss: 0.3410647213459015, train_loss: 0.10195010155439377\n",
      "[po_estimator_0_impute_pos] Epoch: 250, current validation loss: 0.31756511330604553, train_loss: 0.10038580000400543\n",
      "[po_estimator_0_impute_pos] Epoch: 300, current validation loss: 0.2980504035949707, train_loss: 0.057069648057222366\n",
      "[po_estimator_0_impute_pos] Epoch: 350, current validation loss: 0.2826075851917267, train_loss: 0.07447989284992218\n",
      "[po_estimator_0_impute_pos] Epoch: 400, current validation loss: 0.26769503951072693, train_loss: 0.036685992032289505\n",
      "[po_estimator_0_impute_pos] Epoch: 450, current validation loss: 0.25763633847236633, train_loss: 0.03205163776874542\n",
      "[po_estimator_0_impute_pos] Epoch: 500, current validation loss: 0.24549205601215363, train_loss: 0.02683318220078945\n",
      "[po_estimator_0_impute_pos] Epoch: 550, current validation loss: 0.23645809292793274, train_loss: 0.021953165531158447\n",
      "[po_estimator_0_impute_pos] Epoch: 600, current validation loss: 0.22784967720508575, train_loss: 0.03326665237545967\n",
      "[po_estimator_0_impute_pos] Epoch: 650, current validation loss: 0.22080591320991516, train_loss: 0.018790606409311295\n",
      "[po_estimator_0_impute_pos] Epoch: 700, current validation loss: 0.2150512933731079, train_loss: 0.02745465189218521\n",
      "[po_estimator_0_impute_pos] Epoch: 750, current validation loss: 0.20902588963508606, train_loss: 0.02654777280986309\n",
      "[po_estimator_0_impute_pos] Epoch: 800, current validation loss: 0.20289182662963867, train_loss: 0.014678490348160267\n",
      "[po_estimator_0_impute_pos] Epoch: 850, current validation loss: 0.19770728051662445, train_loss: 0.02066473476588726\n",
      "[po_estimator_0_impute_pos] Epoch: 900, current validation loss: 0.1929093301296234, train_loss: 0.01169582549482584\n",
      "[po_estimator_0_impute_pos] Epoch: 950, current validation loss: 0.18930070102214813, train_loss: 0.010702832601964474\n",
      "[po_estimator_0_impute_pos] Epoch: 1000, current validation loss: 0.18414078652858734, train_loss: 0.010486263781785965\n",
      "[po_estimator_0_impute_pos] Epoch: 1050, current validation loss: 0.1804339587688446, train_loss: 0.008710741996765137\n",
      "[po_estimator_0_impute_pos] Epoch: 1100, current validation loss: 0.1764976680278778, train_loss: 0.009109733626246452\n",
      "[po_estimator_0_impute_pos] Epoch: 1150, current validation loss: 0.17420268058776855, train_loss: 0.008670306764543056\n",
      "[po_estimator_0_impute_pos] Epoch: 1200, current validation loss: 0.17106764018535614, train_loss: 0.007134804967790842\n",
      "[po_estimator_0_impute_pos] Epoch: 1250, current validation loss: 0.1671382337808609, train_loss: 0.012047491036355495\n",
      "[po_estimator_0_impute_pos] Epoch: 1300, current validation loss: 0.1645917147397995, train_loss: 0.007037934381514788\n",
      "[po_estimator_0_impute_pos] Epoch: 1350, current validation loss: 0.16144345700740814, train_loss: 0.005936723202466965\n",
      "[po_estimator_0_impute_pos] Epoch: 1400, current validation loss: 0.15793214738368988, train_loss: 0.005956858396530151\n",
      "[po_estimator_0_impute_pos] Epoch: 1450, current validation loss: 0.15542376041412354, train_loss: 0.005660600960254669\n",
      "[po_estimator_1_impute_pos] Epoch: 0, current validation loss: 0.646733820438385, train_loss: 0.5882156491279602\n",
      "[po_estimator_1_impute_pos] Epoch: 50, current validation loss: 0.5583612322807312, train_loss: 0.44439783692359924\n",
      "[po_estimator_1_impute_pos] Epoch: 100, current validation loss: 0.5260697603225708, train_loss: 0.34252646565437317\n",
      "[po_estimator_1_impute_pos] Epoch: 150, current validation loss: 0.5080287456512451, train_loss: 0.26929715275764465\n",
      "[po_estimator_1_impute_pos] Epoch: 200, current validation loss: 0.5033022165298462, train_loss: 0.21481174230575562\n",
      "[po_estimator_0_impute_pos] Epoch: 0, current validation loss: 0.42856431007385254, train_loss: 0.4130479395389557\n",
      "[po_estimator_0_impute_pos] Epoch: 50, current validation loss: 0.41262388229370117, train_loss: 0.2597624957561493\n",
      "[po_estimator_0_impute_pos] Epoch: 100, current validation loss: 0.41432610154151917, train_loss: 0.24346905946731567\n",
      "[po_estimator_0_impute_pos] Epoch: 150, current validation loss: 0.41474780440330505, train_loss: 0.22748009860515594\n",
      "[po_estimator_0_impute_pos] Epoch: 200, current validation loss: 0.4159104526042938, train_loss: 0.2177211493253708\n",
      "[po_estimator_1_impute_pos] Epoch: 0, current validation loss: 0.6906849145889282, train_loss: 0.6716943383216858\n",
      "[po_estimator_1_impute_pos] Epoch: 50, current validation loss: 0.5881391167640686, train_loss: 0.5098941326141357\n",
      "[po_estimator_1_impute_pos] Epoch: 100, current validation loss: 0.5025305151939392, train_loss: 0.39862585067749023\n",
      "[po_estimator_1_impute_pos] Epoch: 150, current validation loss: 0.4365730583667755, train_loss: 0.3037501275539398\n",
      "[po_estimator_1_impute_pos] Epoch: 200, current validation loss: 0.3913871645927429, train_loss: 0.2372046858072281\n",
      "[po_estimator_1_impute_pos] Epoch: 250, current validation loss: 0.3614172041416168, train_loss: 0.202073872089386\n",
      "[po_estimator_1_impute_pos] Epoch: 300, current validation loss: 0.34015578031539917, train_loss: 0.15951725840568542\n",
      "[po_estimator_1_impute_pos] Epoch: 350, current validation loss: 0.3258708417415619, train_loss: 0.13371753692626953\n",
      "[po_estimator_1_impute_pos] Epoch: 400, current validation loss: 0.3156248927116394, train_loss: 0.11414497345685959\n",
      "[po_estimator_1_impute_pos] Epoch: 450, current validation loss: 0.30813756585121155, train_loss: 0.10001729428768158\n",
      "[po_estimator_1_impute_pos] Epoch: 500, current validation loss: 0.3031899929046631, train_loss: 0.09024947136640549\n",
      "[te_estimator_0_xnet] Epoch: 0, current validation loss: 0.699927568435669, train_loss: 0.6290123462677002\n",
      "[te_estimator_0_xnet] Epoch: 50, current validation loss: 0.3530940115451813, train_loss: 0.01196687389165163\n",
      "[te_estimator_0_xnet] Epoch: 100, current validation loss: 0.3379595875740051, train_loss: 0.0010050133569166064\n",
      "[te_estimator_0_xnet] Epoch: 150, current validation loss: 0.33742842078208923, train_loss: 0.001916293054819107\n",
      "[te_estimator_0_xnet] Epoch: 200, current validation loss: 0.33563464879989624, train_loss: 0.00392317958176136\n",
      "[te_estimator_1_xnet] Epoch: 0, current validation loss: 0.5203841328620911, train_loss: 0.4881749749183655\n",
      "[te_estimator_1_xnet] Epoch: 50, current validation loss: 0.2070835530757904, train_loss: 0.08425603806972504\n",
      "[te_estimator_1_xnet] Epoch: 100, current validation loss: 0.18876038491725922, train_loss: 0.049373410642147064\n",
      "[te_estimator_1_xnet] Epoch: 150, current validation loss: 0.18812009692192078, train_loss: 0.04301692545413971\n",
      "[te_estimator_1_xnet] Epoch: 200, current validation loss: 0.1875593066215515, train_loss: 0.03830501809716225\n",
      "[po_estimator_0_impute_pos] Epoch: 0, current validation loss: 0.7646424174308777, train_loss: 0.6840691566467285\n",
      "[po_estimator_0_impute_pos] Epoch: 50, current validation loss: 0.6430924534797668, train_loss: 0.4381125271320343\n",
      "[po_estimator_0_impute_pos] Epoch: 100, current validation loss: 0.524880051612854, train_loss: 0.2029212862253189\n",
      "[po_estimator_0_impute_pos] Epoch: 150, current validation loss: 0.45412522554397583, train_loss: 0.12272870540618896\n",
      "[po_estimator_0_impute_pos] Epoch: 200, current validation loss: 0.4042971134185791, train_loss: 0.09990042448043823\n",
      "[po_estimator_0_impute_pos] Epoch: 250, current validation loss: 0.3715429902076721, train_loss: 0.13670773804187775\n",
      "[po_estimator_0_impute_pos] Epoch: 300, current validation loss: 0.33519086241722107, train_loss: 0.05446000397205353\n",
      "[po_estimator_0_impute_pos] Epoch: 350, current validation loss: 0.3107125759124756, train_loss: 0.044989295303821564\n",
      "[po_estimator_0_impute_pos] Epoch: 400, current validation loss: 0.29217347502708435, train_loss: 0.07503540068864822\n",
      "[po_estimator_0_impute_pos] Epoch: 450, current validation loss: 0.2752067744731903, train_loss: 0.03412666544318199\n",
      "[po_estimator_0_impute_pos] Epoch: 500, current validation loss: 0.2634277939796448, train_loss: 0.03027082234621048\n",
      "[po_estimator_0_impute_pos] Epoch: 550, current validation loss: 0.25005480647087097, train_loss: 0.046937499195337296\n",
      "[po_estimator_0_impute_pos] Epoch: 600, current validation loss: 0.23876319825649261, train_loss: 0.04264677315950394\n",
      "[po_estimator_0_impute_pos] Epoch: 650, current validation loss: 0.22880832850933075, train_loss: 0.021478278562426567\n",
      "[po_estimator_0_impute_pos] Epoch: 700, current validation loss: 0.22019898891448975, train_loss: 0.036815445870161057\n",
      "[po_estimator_0_impute_pos] Epoch: 750, current validation loss: 0.2122092992067337, train_loss: 0.01650644838809967\n",
      "[po_estimator_0_impute_pos] Epoch: 800, current validation loss: 0.20420365035533905, train_loss: 0.01486972626298666\n",
      "[po_estimator_0_impute_pos] Epoch: 850, current validation loss: 0.19638536870479584, train_loss: 0.013596591539680958\n",
      "[po_estimator_0_impute_pos] Epoch: 900, current validation loss: 0.1905832141637802, train_loss: 0.024834997951984406\n",
      "[po_estimator_0_impute_pos] Epoch: 950, current validation loss: 0.18471553921699524, train_loss: 0.011585807427763939\n",
      "[po_estimator_0_impute_pos] Epoch: 1000, current validation loss: 0.17972679436206818, train_loss: 0.010941405780613422\n",
      "[po_estimator_0_impute_pos] Epoch: 1050, current validation loss: 0.17527256906032562, train_loss: 0.010576865635812283\n",
      "[po_estimator_0_impute_pos] Epoch: 1100, current validation loss: 0.17060914635658264, train_loss: 0.009331556037068367\n",
      "[po_estimator_0_impute_pos] Epoch: 1150, current validation loss: 0.165763720870018, train_loss: 0.00970155093818903\n",
      "[po_estimator_0_impute_pos] Epoch: 1200, current validation loss: 0.16107521951198578, train_loss: 0.008484873920679092\n",
      "[po_estimator_0_impute_pos] Epoch: 1250, current validation loss: 0.15698468685150146, train_loss: 0.008799171075224876\n",
      "[po_estimator_0_impute_pos] Epoch: 1300, current validation loss: 0.1535271853208542, train_loss: 0.014156554825603962\n",
      "[po_estimator_0_impute_pos] Epoch: 1350, current validation loss: 0.14922727644443512, train_loss: 0.007344376761466265\n",
      "[po_estimator_0_impute_pos] Epoch: 1400, current validation loss: 0.14629507064819336, train_loss: 0.006794815883040428\n",
      "[po_estimator_0_impute_pos] Epoch: 1450, current validation loss: 0.1427379697561264, train_loss: 0.006286181975156069\n",
      "[po_estimator_1_impute_pos] Epoch: 0, current validation loss: 0.6355690360069275, train_loss: 0.6044817566871643\n",
      "[po_estimator_1_impute_pos] Epoch: 50, current validation loss: 0.563497006893158, train_loss: 0.4694939851760864\n",
      "[po_estimator_1_impute_pos] Epoch: 100, current validation loss: 0.5206990838050842, train_loss: 0.3582228124141693\n",
      "[po_estimator_1_impute_pos] Epoch: 150, current validation loss: 0.49801188707351685, train_loss: 0.28902846574783325\n",
      "[po_estimator_1_impute_pos] Epoch: 200, current validation loss: 0.4892215430736542, train_loss: 0.2455473244190216\n",
      "[po_estimator_0_impute_pos] Epoch: 0, current validation loss: 0.48057547211647034, train_loss: 0.4686189293861389\n",
      "[po_estimator_0_impute_pos] Epoch: 50, current validation loss: 0.4442327618598938, train_loss: 0.30606359243392944\n",
      "[po_estimator_0_impute_pos] Epoch: 100, current validation loss: 0.422675222158432, train_loss: 0.20721159875392914\n",
      "[po_estimator_0_impute_pos] Epoch: 150, current validation loss: 0.41376993060112, train_loss: 0.1549639105796814\n",
      "[po_estimator_0_impute_pos] Epoch: 200, current validation loss: 0.4126124382019043, train_loss: 0.12770190834999084\n",
      "[po_estimator_1_impute_pos] Epoch: 0, current validation loss: 0.9065665602684021, train_loss: 0.8976401090621948\n",
      "[po_estimator_1_impute_pos] Epoch: 50, current validation loss: 0.7378700375556946, train_loss: 0.6748374700546265\n",
      "[po_estimator_1_impute_pos] Epoch: 100, current validation loss: 0.624074399471283, train_loss: 0.5195478200912476\n",
      "[po_estimator_1_impute_pos] Epoch: 150, current validation loss: 0.5383540987968445, train_loss: 0.40263307094573975\n",
      "[po_estimator_1_impute_pos] Epoch: 200, current validation loss: 0.46772485971450806, train_loss: 0.3192790746688843\n",
      "[po_estimator_1_impute_pos] Epoch: 250, current validation loss: 0.416786253452301, train_loss: 0.2549731135368347\n",
      "[po_estimator_1_impute_pos] Epoch: 300, current validation loss: 0.38372802734375, train_loss: 0.2004663050174713\n",
      "[po_estimator_1_impute_pos] Epoch: 350, current validation loss: 0.3607824444770813, train_loss: 0.17194880545139313\n",
      "[po_estimator_1_impute_pos] Epoch: 400, current validation loss: 0.3428308367729187, train_loss: 0.1429724544286728\n",
      "[po_estimator_1_impute_pos] Epoch: 450, current validation loss: 0.33067846298217773, train_loss: 0.11914296448230743\n",
      "[po_estimator_1_impute_pos] Epoch: 500, current validation loss: 0.3214975595474243, train_loss: 0.10572691261768341\n",
      "[po_estimator_1_impute_pos] Epoch: 550, current validation loss: 0.3165712058544159, train_loss: 0.09024135768413544\n",
      "[po_estimator_1_impute_pos] Epoch: 600, current validation loss: 0.31045156717300415, train_loss: 0.08110980689525604\n",
      "[te_estimator] Epoch: 0, current validation loss: 2.9318161010742188, train_loss: 4.989114284515381\n",
      "[te_estimator] Epoch: 50, current validation loss: 2.5757699012756348, train_loss: 4.408824443817139\n",
      "[te_estimator] Epoch: 100, current validation loss: 2.5849742889404297, train_loss: 3.841562271118164\n",
      "[te_estimator] Epoch: 150, current validation loss: 2.5945749282836914, train_loss: 3.519237518310547\n",
      "[te_estimator] Epoch: 200, current validation loss: 2.6039085388183594, train_loss: 4.241492748260498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DRLearner(\n",
       "  (_te_estimator): BasicNet(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=46, out_features=100, bias=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=100, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_po_estimator): BasicNet(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=46, out_features=100, bias=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=100, out_features=1, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (_propensity_estimator): PropensityNet(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=46, out_features=100, bias=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=100, out_features=2, bias=True)\n",
       "      (4): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fluid_cohort = pd.read_pickle(\"../data/trauma_responder.pkl\")\n",
    "\n",
    "\n",
    "all_year = pd.read_csv(\"../data/all_year.csv\", index_col=0)\n",
    "\n",
    "fluid_cohort = pd.merge(fluid_cohort,all_year[['registryid','iss']],on='registryid', how='left')\n",
    "fluid_cohort[\"iss\"] = pd.to_numeric(fluid_cohort[\"iss\"], errors='coerce')\n",
    "\n",
    "#\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='proc')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='ethnicity')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='residencestate')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='toxicologyresults')))]\n",
    "\n",
    "\n",
    "x = fluid_cohort.loc[:, ~fluid_cohort.columns.isin([\"registryid\",\n",
    "                                                            \"COV\",\n",
    "                                                            \"TT\",\n",
    "                                                            \"scenegcsmotor\",\n",
    "                                                            \"scenegcseye\",\n",
    "                                                            \"scenegcsverbal\",\n",
    "                                                            \"edgcsmotor\",\n",
    "                                                            \"edgcseye\",\n",
    "                                                            \"edgcsverbal\",\n",
    "                                                            \"outcome\",\n",
    "                                                            \"sex_F\",\n",
    "                                                            \"traumatype_P\",\n",
    "                                                            \"traumatype_other\"\n",
    "                                                            ])]\n",
    "\n",
    "n, feature_size = x.shape\n",
    "names = x.drop([\"treated\"], axis=1).columns\n",
    "treatment_index = x.columns.get_loc(\"treated\")\n",
    "iss_index = x.columns.get_loc(\"iss\")\n",
    "sex_index = x.columns.get_loc(\"sex_M\")\n",
    "\n",
    "var_index = [i for i in range(feature_size) if i != treatment_index and i != iss_index]\n",
    "\n",
    "x_norm = normalize_data(x)\n",
    "\n",
    "## impute missing value\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(x_norm)\n",
    "x_train_scaled = imp.transform(x_norm)\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "                                             x_train_scaled,  \n",
    "                                             fluid_cohort[\"outcome\"], \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=10,\n",
    "                                             stratify=fluid_cohort[\"treated\"]\n",
    "                                    )\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "                                             x_train,  \n",
    "                                             y_train, \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=10,\n",
    "                                             stratify=x_train[:, treatment_index]\n",
    "                                    )\n",
    "\n",
    "w_train = x_train[:, treatment_index]\n",
    "w_val = x_val[:, treatment_index]\n",
    "w_test =  x_test[:, treatment_index]\n",
    "\n",
    "\n",
    "iss_train = x_train[:, iss_index]\n",
    "iss_test =  x_test[:, iss_index]\n",
    "\n",
    "x_train = x_train[:,var_index]\n",
    "x_val = x_val[:, var_index]\n",
    "x_test = x_test[:, var_index]\n",
    "\n",
    "x_learner = pseudo_outcome_nets.XLearner( \n",
    "    \n",
    "    x_train.shape[1],\n",
    "    binary_y=(len(np.unique(y_train)) == 2),\n",
    "    n_layers_out=2,\n",
    "    n_units_out=100,\n",
    "    batch_size=128,\n",
    "    n_iter=1500,\n",
    "    nonlin=\"relu\",\n",
    "    device=\"cuda:1\",\n",
    ")\n",
    "dr_learner = pseudo_outcome_nets.DRLearner( \n",
    "    \n",
    "    x_train.shape[1],\n",
    "    binary_y=(len(np.unique(y_train)) == 2),\n",
    "    n_layers_out=2,\n",
    "    n_units_out=100,\n",
    "    batch_size=128,\n",
    "    n_iter=1500,\n",
    "    nonlin=\"relu\",\n",
    "    device=\"cuda:1\",\n",
    ")\n",
    "\n",
    "\n",
    "x_learner.fit(x_train, y_train, w_train)\n",
    "dr_learner.fit(x_train, y_train, w_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9871b8-6526-4242-ae7e-1e558f89c4c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Debug DR\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_nuisance_models\u001b[39m(\n\u001b[0;32m----> 4\u001b[0m     x_val: \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m      5\u001b[0m     y_val: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m      6\u001b[0m     w_val: np\u001b[38;5;241m.\u001b[39mndarray\n\u001b[1;32m      7\u001b[0m )\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m:\n\u001b[1;32m      9\u001b[0m     mu0 \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier()\n\u001b[1;32m     10\u001b[0m     mu1 \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "## Debug DR\n",
    "\n",
    "def train_nuisance_models(\n",
    "    x_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    w_val: np.ndarray\n",
    ")-> tuple:\n",
    "\n",
    "    mu0 = xgb.XGBClassifier()\n",
    "    mu1 = xgb.XGBClassifier()\n",
    "    m = xgb.XGBClassifier()\n",
    "    rf = xgb.XGBClassifier(\n",
    "        # reg_lambda=2,\n",
    "        # max_depth=3,\n",
    "        # colsample_bytree=0.2,\n",
    "        # min_split_loss=10\n",
    "    )\n",
    "\n",
    "    x0, x1 = x_val[w_val == 0], x_val[w_val == 1]\n",
    "    y0, y1 = y_val[w_val == 0], y_val[w_val == 1]\n",
    "\n",
    "    mu0.fit(x0, y0)\n",
    "    mu1.fit(x1, y1)\n",
    "    m.fit(x_val, y_val)\n",
    "    rf.fit(x_val, w_val)\n",
    "\n",
    "    return mu0, mu1, rf, m\n",
    "\n",
    "def calculate_if_pehe(\n",
    "    w_test: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    prediction: np.ndarray,\n",
    "    t_plugin: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    ident: np.ndarray\n",
    ")-> np.ndarray:\n",
    "\n",
    "    EPS = 1e-7\n",
    "    a = w_test - p\n",
    "    c = p * (ident - p)\n",
    "    b = 2 * np.ones(len(w_test)) * w_test * (w_test - p) / (c + EPS)\n",
    "\n",
    "    plug_in = (t_plugin - prediction) ** 2\n",
    "    l_de = (ident - b) * t_plugin ** 2 + b * y_test * (t_plugin - prediction) + (- a * (t_plugin - prediction) ** 2 + prediction ** 2)\n",
    "\n",
    "    return np.sum(plug_in) + np.sum(l_de)\n",
    "\n",
    "\n",
    "def calculate_pseudo_outcome_pehe_dr(\n",
    "    w_test: np.ndarray,\n",
    "    p: np.ndarray,\n",
    "    prediction: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    mu_1: np.ndarray,\n",
    "    mu_0: np.ndarray\n",
    ")-> np.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "    calculating pseudo outcome for DR\n",
    "    \"\"\"\n",
    "    # p = 0.5*np.ones(len(w_test))\n",
    "    \n",
    "    EPS = 1e-7\n",
    "    w_1 = w_test / (p + EPS)\n",
    "    w_0 = (1 - w_test) / (EPS + 1 - p)\n",
    "    \n",
    "    pseudo_outcome = (w_1 - w_0) * y_test + ((1 - w_1) * mu_1 - (1 - w_0) * mu_0)\n",
    "        \n",
    "    # pseudo_outcome = np.clip(pseudo_outcome, -1, 1)\n",
    "    return np.sqrt(np.mean((prediction - pseudo_outcome) ** 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "c9abfb6b-ea8f-4535-ba0e-21a72eceb0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC for propensity model 0.5359154929577464\n",
      "-0.037747584 0.09703427\n",
      "pehe_dr: xlearner 63.994768524680374\n",
      "pehe_dr: drlearner 64.02349958412289\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "xgb_plugin0, xgb_plugin1, rf, m = train_nuisance_models(x_train, y_train, w_train)\n",
    "\n",
    "mu_0 = xgb_plugin0.predict_proba(x_test)[:, 1]\n",
    "mu_1 = xgb_plugin1.predict_proba(x_test)[:, 1]\n",
    "\n",
    "mu = m.predict_proba(x_test)[:, 1]\n",
    "p = rf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "print(\"AUROC for propensity model\", roc_auc_score(w_test, p))\n",
    "\n",
    "t_plugin = mu_1 - mu_0\n",
    "ident = np.ones(len(p))\n",
    "\n",
    "x_prediction = x_learner.predict(X=x_test).detach().cpu().numpy().flatten()\n",
    "dr_prediction = dr_learner.predict(X=x_test).detach().cpu().numpy().flatten()\n",
    "\n",
    "print(np.mean(x_prediction), np.mean(dr_prediction))\n",
    "\n",
    "print(\"pehe_dr: xlearner\", calculate_pseudo_outcome_pehe_dr(w_test, p, x_prediction, y_test, mu_1, mu_0))\n",
    "print(\"pehe_dr: drlearner\", calculate_pseudo_outcome_pehe_dr(w_test, p, dr_prediction, y_test, mu_1, mu_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a564a210-0143-46d2-b58a-9d635659dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/gws/mingyulu/miniconda3/envs/torch_gpu/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import umap\n",
    "import umap.plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../CATENets/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import catenets.models.torch.pseudo_outcome_nets as pseudo_outcome_nets\n",
    "\n",
    "\n",
    "def normalize_data(x_train):\n",
    "    \n",
    "    x_normalized_train = (x_train - np.min(x_train, axis=0)) / (np.max(x_train, axis=0) - np.min(x_train, axis=0))\n",
    "\n",
    "    return x_normalized_train\n",
    "\n",
    "\n",
    "def subgroup_ate(\n",
    "    method: str,\n",
    "    features: List[int],\n",
    "    y_true_train: np.ndarray,\n",
    "    y_true_test: np.ndarray,\n",
    "    estimated_ate_test: np.ndarray,\n",
    "    iss_test: np.ndarray\n",
    ") -> None:\n",
    "    \n",
    "    xgb_model = xgb.XGBClassifier(  \n",
    "        max_depth=5,\n",
    "        reg_lambda=2,\n",
    "        min_split_loss=2\n",
    "    )\n",
    "    \n",
    "    # print(len(y_true_train), np.sum(y_true_train))\n",
    "    # print(len(y_true_test), np.sum(y_true_test))\n",
    "\n",
    "    xgb_model.fit(x_train[:, features], y_true_train)\n",
    "\n",
    "    y_pred = xgb_model.predict(x_test[:, features])\n",
    "    \n",
    "    ate = np.sum(estimated_ate_test[y_pred == 1])/n\n",
    "\n",
    "    auroc = metrics.roc_auc_score(y_true_test, y_pred)\n",
    "\n",
    "    print(\"%s - auroc %s\"%(method, auroc))\n",
    "    print(\"%s - ATE %s\"%(method, ate))\n",
    "    \n",
    "    # # if iss_test.any():  \n",
    "    # print(\"%s - iss %s; %s \"%(method , np.mean(iss_test[y_pred==1])*74+1, np.mean(iss_test[y_pred==0])*74+1))\n",
    "    # print(\"===================================\")\n",
    "\n",
    "def feature_idx(\n",
    "    method: str,\n",
    "    cohort: str\n",
    ")-> List[int]:\n",
    "    \n",
    "    if method == \"shap\":\n",
    "        file_path = f\"../results/{cohort}/naive_shap_top_5_features_xlearner.csv\"\n",
    "    elif method == \"ig\":\n",
    "        file_path = f\"../results/{cohort}/integrated_gradients_top_5_features_xlearner.csv\"\n",
    "    elif method == \"shap - 0 \":\n",
    "        file_path = f\"../results/{cohort}/shapley_value_sampling_top_5_features_xlearner.csv\"\n",
    "        \n",
    "    df = pd.read_csv(file_path,keep_default_na=False)\n",
    "    df_sorted = df.sort_values(\n",
    "        by='count (%)', \n",
    "        ascending=False\n",
    "    )\n",
    "    print(df_sorted[\"feature\"].head(5).tolist() )\n",
    "    return [ x.columns.get_loc(col) for col in df_sorted[\"feature\"].head(5) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ffa0d321-dfba-410d-82e4-2fa97fe0b9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0031752574660565665 0.02006738920686732\n",
      "-0.0062600657035593605 0.03674617458458685\n",
      "['temps2', 'scenefirstbloodpressure', 'NA', 'sex_M', 'HGB']\n",
      "['temps2', 'sex_M', 'NA', 'HGB', 'traumatype_B']\n",
      "['edgcs', 'sex_M', 'scenegcs', 'traumatype_B', 'causecode_CUT']\n",
      "mean ISS:  29.70394736842105\n",
      "original -0.0015422262866046938\n",
      "original - iss 29.774647887323944 28.699999999999996\n",
      "===================================\n",
      "shap - 0  - auroc 0.5270833333333332\n",
      "shap - 0  - ATE 0.000612887897694665\n",
      "ig - auroc 0.5208333333333333\n",
      "ig - ATE -0.0007237388354226736\n",
      "shap - auroc 0.45486111111111116\n",
      "shap - ATE -0.0018888624825736334\n",
      "clinical - auroc 0.4708333333333333\n",
      "clinical - ATE -0.001613861829016667\n",
      "full features - auroc 0.5506944444444444\n",
      "full features - ATE 0.000509899799113514\n",
      "random features - auroc 0.4756944444444444\n",
      "random features - ATE -0.002076809083033974\n"
     ]
    }
   ],
   "source": [
    "results_train = pickle.load(open(\"../results/responder/train_xlearner.pkl\", \"rb\"))\n",
    "results_test = pickle.load(open(\"../results/responder/test_xlearner.pkl\", \"rb\"))\n",
    "\n",
    "print(np.mean(results_train), np.std(results_train)/np.sqrt(results_train.shape[1]))\n",
    "print(np.mean(results_test), np.std(results_test)/np.sqrt(results_test.shape[1]))\n",
    "\n",
    "estimated_ate_train = np.mean(results_train, axis=0)\n",
    "estimated_ate_test = np.mean(results_test, axis=0)\n",
    "threshold = np.mean(estimated_ate_train)\n",
    "\n",
    "y_true_train = (estimated_ate_train > threshold)\n",
    "y_true_test = (estimated_ate_test > threshold)\n",
    "\n",
    "scenefirstbloodpressure = x.columns.get_loc(\"scenefirstbloodpressure\")\n",
    "lac  = x.columns.get_loc(\"LAC\")\n",
    "inr  = x.columns.get_loc(\"INR\")\n",
    "hgb  = x.columns.get_loc(\"HGB\")\n",
    "\n",
    "explainers = {\n",
    "    \"shap - 0 \": feature_idx(\"shap - 0 \",\"responder\" ), #[temp, ph, bd, hgb, pulse ]\n",
    "    \"ig\": feature_idx(\"ig\",\"responder\" ), #[ph, na, temp, gender, fio2 ],\n",
    "    \"shap\": feature_idx(\"shap\",\"responder\"),\n",
    "\n",
    "    \n",
    "    \"clinical\": [lac, inr, hgb,scenefirstbloodpressure ],\n",
    "    \"full features\": [ i for i in range(x_train.shape[1])],\n",
    "    \"random features\": np.random.randint(x_train.shape[1], size=(5)),\n",
    "}\n",
    "\n",
    "\n",
    "print(\"mean ISS: \", np.mean(iss_test)*74+1)\n",
    "print(\"original\", np.sum(estimated_ate_test[w_test==1])/n)\n",
    "print(\"original - iss\", np.mean(iss_test[w_test==1])*74+1, np.mean(iss_test[w_test==0])*74+1)\n",
    "print(\"===================================\")\n",
    "\n",
    "for explainer, features in explainers.items():\n",
    "    subgroup_ate(\n",
    "        explainer,\n",
    "        features,\n",
    "        y_true_train,\n",
    "        y_true_test,\n",
    "        estimated_ate_test,\n",
    "        iss_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d310673-83d3-4ec2-a5c7-682ee9462cc9",
   "metadata": {},
   "source": [
    "## Transfusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "0c9b4841-7234-4ce3-957f-82991b765a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40033/4013390563.py:3: DtypeWarning: Columns (9,16,45,57,58,59,60,61,62,63,64,71,84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_year = pd.read_csv(\"../data/all_year.csv\", index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[po_estimator_0_impute_pos] Epoch: 0, current validation loss: 0.6170044541358948, train_loss: 0.6072909832000732\n",
      "[po_estimator_0_impute_pos] Epoch: 50, current validation loss: 0.5410065054893494, train_loss: 0.41597023606300354\n",
      "[po_estimator_0_impute_pos] Epoch: 100, current validation loss: 0.5091976523399353, train_loss: 0.3120671510696411\n",
      "[po_estimator_0_impute_pos] Epoch: 150, current validation loss: 0.4964032769203186, train_loss: 0.24495665729045868\n",
      "[po_estimator_0_impute_pos] Epoch: 200, current validation loss: 0.4935840666294098, train_loss: 0.20134633779525757\n",
      "[po_estimator_1_impute_pos] Epoch: 0, current validation loss: 0.8237159848213196, train_loss: 0.736664354801178\n",
      "[po_estimator_1_impute_pos] Epoch: 50, current validation loss: 0.6641184091567993, train_loss: 0.5368849039077759\n",
      "[po_estimator_1_impute_pos] Epoch: 100, current validation loss: 0.5966092348098755, train_loss: 0.41602587699890137\n",
      "[po_estimator_1_impute_pos] Epoch: 150, current validation loss: 0.5716368556022644, train_loss: 0.3429119884967804\n",
      "[po_estimator_1_impute_pos] Epoch: 200, current validation loss: 0.5608240962028503, train_loss: 0.2811158001422882\n",
      "[po_estimator_1_impute_pos] Epoch: 250, current validation loss: 0.557991087436676, train_loss: 0.23029235005378723\n",
      "[po_estimator_1_impute_pos] Epoch: 300, current validation loss: 0.556691586971283, train_loss: 0.20393024384975433\n",
      "[po_estimator_0_impute_pos] Epoch: 0, current validation loss: 0.6511595249176025, train_loss: 0.6961124539375305\n",
      "[po_estimator_0_impute_pos] Epoch: 50, current validation loss: 0.5101348757743835, train_loss: 0.47867175936698914\n",
      "[po_estimator_0_impute_pos] Epoch: 100, current validation loss: 0.4369261860847473, train_loss: 0.36531171202659607\n",
      "[po_estimator_0_impute_pos] Epoch: 150, current validation loss: 0.3922182619571686, train_loss: 0.2879045009613037\n",
      "[po_estimator_0_impute_pos] Epoch: 200, current validation loss: 0.35968807339668274, train_loss: 0.23095852136611938\n",
      "[po_estimator_0_impute_pos] Epoch: 250, current validation loss: 0.33533287048339844, train_loss: 0.18753552436828613\n",
      "[po_estimator_0_impute_pos] Epoch: 300, current validation loss: 0.31809476017951965, train_loss: 0.15445828437805176\n",
      "[po_estimator_0_impute_pos] Epoch: 350, current validation loss: 0.30600032210350037, train_loss: 0.1278797835111618\n",
      "[po_estimator_0_impute_pos] Epoch: 400, current validation loss: 0.2968388795852661, train_loss: 0.1054270789027214\n",
      "[po_estimator_0_impute_pos] Epoch: 450, current validation loss: 0.2910759747028351, train_loss: 0.0876338854432106\n",
      "[po_estimator_0_impute_pos] Epoch: 500, current validation loss: 0.287039190530777, train_loss: 0.07344284653663635\n",
      "[po_estimator_0_impute_pos] Epoch: 550, current validation loss: 0.28468358516693115, train_loss: 0.06336117535829544\n",
      "[po_estimator_1_impute_pos] Epoch: 0, current validation loss: 0.7195150852203369, train_loss: 0.7066149711608887\n",
      "[po_estimator_1_impute_pos] Epoch: 50, current validation loss: 0.6240269541740417, train_loss: 0.47281160950660706\n",
      "[po_estimator_1_impute_pos] Epoch: 100, current validation loss: 0.5792978405952454, train_loss: 0.35113298892974854\n",
      "[po_estimator_1_impute_pos] Epoch: 150, current validation loss: 0.5587249994277954, train_loss: 0.2699979245662689\n",
      "[po_estimator_1_impute_pos] Epoch: 200, current validation loss: 0.5501881837844849, train_loss: 0.21059489250183105\n",
      "[te_estimator_0_xnet] Epoch: 0, current validation loss: 0.3256741464138031, train_loss: 0.3765864968299866\n",
      "[te_estimator_0_xnet] Epoch: 50, current validation loss: 0.19625991582870483, train_loss: 0.08589017391204834\n",
      "[te_estimator_0_xnet] Epoch: 100, current validation loss: 0.19229480624198914, train_loss: 0.05914102494716644\n",
      "[te_estimator_0_xnet] Epoch: 150, current validation loss: 0.19103600084781647, train_loss: 0.057361237704753876\n",
      "[te_estimator_0_xnet] Epoch: 200, current validation loss: 0.19048386812210083, train_loss: 0.05015727877616882\n",
      "[te_estimator_1_xnet] Epoch: 0, current validation loss: 0.5454234480857849, train_loss: 0.5736185312271118\n",
      "[te_estimator_1_xnet] Epoch: 50, current validation loss: 0.25012487173080444, train_loss: 0.10335673391819\n",
      "[te_estimator_1_xnet] Epoch: 100, current validation loss: 0.24724729359149933, train_loss: 0.062016114592552185\n",
      "[te_estimator_1_xnet] Epoch: 150, current validation loss: 0.24896639585494995, train_loss: 0.05698564648628235\n",
      "[te_estimator_1_xnet] Epoch: 200, current validation loss: 0.2500448226928711, train_loss: 0.04884037375450134\n",
      "[po_estimator_0_impute_pos] Epoch: 0, current validation loss: 0.8957683444023132, train_loss: 0.9257713556289673\n",
      "[po_estimator_0_impute_pos] Epoch: 50, current validation loss: 0.7187185287475586, train_loss: 0.6388452053070068\n",
      "[po_estimator_0_impute_pos] Epoch: 100, current validation loss: 0.6233245730400085, train_loss: 0.4401443898677826\n",
      "[po_estimator_0_impute_pos] Epoch: 150, current validation loss: 0.5658403635025024, train_loss: 0.31539079546928406\n",
      "[po_estimator_0_impute_pos] Epoch: 200, current validation loss: 0.5358247756958008, train_loss: 0.24863576889038086\n",
      "[po_estimator_0_impute_pos] Epoch: 250, current validation loss: 0.5233621597290039, train_loss: 0.1894538551568985\n",
      "[po_estimator_0_impute_pos] Epoch: 300, current validation loss: 0.5196356773376465, train_loss: 0.15352681279182434\n",
      "[po_estimator_1_impute_pos] Epoch: 0, current validation loss: 0.8344066143035889, train_loss: 0.8113164305686951\n",
      "[po_estimator_1_impute_pos] Epoch: 50, current validation loss: 0.7090271711349487, train_loss: 0.5856087803840637\n",
      "[po_estimator_1_impute_pos] Epoch: 100, current validation loss: 0.6611470580101013, train_loss: 0.4455392062664032\n",
      "[po_estimator_1_impute_pos] Epoch: 150, current validation loss: 0.6473953127861023, train_loss: 0.3598968982696533\n",
      "[po_estimator_1_impute_pos] Epoch: 200, current validation loss: 0.638439416885376, train_loss: 0.30011099576950073\n",
      "[po_estimator_1_impute_pos] Epoch: 250, current validation loss: 0.6340035200119019, train_loss: 0.2448023557662964\n",
      "[po_estimator_0_impute_pos] Epoch: 0, current validation loss: 1.0650187730789185, train_loss: 0.982358455657959\n",
      "[po_estimator_0_impute_pos] Epoch: 50, current validation loss: 0.8253166675567627, train_loss: 0.7012022137641907\n",
      "[po_estimator_0_impute_pos] Epoch: 100, current validation loss: 0.676773726940155, train_loss: 0.4951324164867401\n",
      "[po_estimator_0_impute_pos] Epoch: 150, current validation loss: 0.571989119052887, train_loss: 0.35509955883026123\n",
      "[po_estimator_0_impute_pos] Epoch: 200, current validation loss: 0.5050829648971558, train_loss: 0.2748996913433075\n",
      "[po_estimator_0_impute_pos] Epoch: 250, current validation loss: 0.4629057049751282, train_loss: 0.2133384495973587\n",
      "[po_estimator_0_impute_pos] Epoch: 300, current validation loss: 0.4356077313423157, train_loss: 0.16936148703098297\n",
      "[po_estimator_0_impute_pos] Epoch: 350, current validation loss: 0.41503962874412537, train_loss: 0.13909725844860077\n",
      "[po_estimator_0_impute_pos] Epoch: 400, current validation loss: 0.3995400667190552, train_loss: 0.11522655189037323\n",
      "[po_estimator_0_impute_pos] Epoch: 450, current validation loss: 0.3875029385089874, train_loss: 0.09646941721439362\n",
      "[po_estimator_0_impute_pos] Epoch: 500, current validation loss: 0.3790118098258972, train_loss: 0.08309690654277802\n",
      "[po_estimator_0_impute_pos] Epoch: 550, current validation loss: 0.3725927174091339, train_loss: 0.07047447562217712\n",
      "[po_estimator_0_impute_pos] Epoch: 600, current validation loss: 0.3669827878475189, train_loss: 0.0608464740216732\n",
      "[po_estimator_0_impute_pos] Epoch: 650, current validation loss: 0.3622804880142212, train_loss: 0.052919771522283554\n",
      "[po_estimator_0_impute_pos] Epoch: 700, current validation loss: 0.3589456379413605, train_loss: 0.04635138064622879\n",
      "[po_estimator_0_impute_pos] Epoch: 750, current validation loss: 0.35603734850883484, train_loss: 0.04094681516289711\n",
      "[po_estimator_0_impute_pos] Epoch: 800, current validation loss: 0.3538558781147003, train_loss: 0.03627318516373634\n",
      "[po_estimator_0_impute_pos] Epoch: 850, current validation loss: 0.35249948501586914, train_loss: 0.03241642937064171\n",
      "[po_estimator_0_impute_pos] Epoch: 900, current validation loss: 0.35096997022628784, train_loss: 0.029203860089182854\n",
      "[po_estimator_0_impute_pos] Epoch: 950, current validation loss: 0.3502388000488281, train_loss: 0.02719314582645893\n",
      "[po_estimator_0_impute_pos] Epoch: 1000, current validation loss: 0.3499230444431305, train_loss: 0.02634642831981182\n",
      "[po_estimator_1_impute_pos] Epoch: 0, current validation loss: 0.6609920263290405, train_loss: 0.5977370142936707\n",
      "[po_estimator_1_impute_pos] Epoch: 50, current validation loss: 0.6042950749397278, train_loss: 0.4306265413761139\n",
      "[po_estimator_1_impute_pos] Epoch: 100, current validation loss: 0.5721147656440735, train_loss: 0.32994797825813293\n",
      "[po_estimator_1_impute_pos] Epoch: 150, current validation loss: 0.5544921159744263, train_loss: 0.2506566643714905\n",
      "[po_estimator_1_impute_pos] Epoch: 200, current validation loss: 0.5496653318405151, train_loss: 0.1951318085193634\n",
      "[te_estimator] Epoch: 0, current validation loss: 1.7226893901824951, train_loss: 1.742924451828003\n",
      "[te_estimator] Epoch: 50, current validation loss: 0.8822927474975586, train_loss: 0.6461614370346069\n",
      "[te_estimator] Epoch: 100, current validation loss: 0.8952617645263672, train_loss: 0.5406477451324463\n",
      "[te_estimator] Epoch: 150, current validation loss: 0.90178382396698, train_loss: 0.5400667190551758\n",
      "[te_estimator] Epoch: 200, current validation loss: 0.9076293706893921, train_loss: 0.5362838506698608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DRLearner(\n",
       "  (_te_estimator): BasicNet(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=51, out_features=100, bias=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=100, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (_po_estimator): BasicNet(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=51, out_features=100, bias=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=100, out_features=100, bias=True)\n",
       "      (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=100, out_features=1, bias=True)\n",
       "      (7): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (_propensity_estimator): PropensityNet(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=51, out_features=100, bias=True)\n",
       "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=100, out_features=2, bias=True)\n",
       "      (4): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fluid_cohort = pd.read_pickle(\"../data/low_bp_survival.pkl\")\n",
    "\n",
    "all_year = pd.read_csv(\"../data/all_year.csv\", index_col=0)\n",
    "\n",
    "fluid_cohort = pd.merge(fluid_cohort,all_year[['registryid','iss']],on='registryid', how='left')\n",
    "fluid_cohort[\"iss\"] = pd.to_numeric(fluid_cohort[\"iss\"], errors='coerce')\n",
    "\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='proc')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='ethnicity')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='residencestate')))]\n",
    "fluid_cohort = fluid_cohort[fluid_cohort.columns.drop(list(fluid_cohort.filter(regex='toxicologyresults')))]\n",
    "\n",
    "\n",
    "x = fluid_cohort.loc[:, ~fluid_cohort.columns.isin([\"registryid\",\n",
    "                                                    \"COV\",\n",
    "                                                    \"TT\",\n",
    "                                                    \"scenegcsmotor\",\n",
    "                                                    \"scenegcseye\",\n",
    "                                                    \"scenegcsverbal\",\n",
    "                                                    \"edgcsmotor\",\n",
    "                                                    \"edgcseye\",\n",
    "                                                    \"edgcsverbal\",\n",
    "                                                    \"outcome\",\n",
    "                                                    \"sex_F\",\n",
    "                                                    \"traumatype_P\",\n",
    "                                                    \"traumatype_other\",\n",
    "                                                ])\n",
    "                    ]\n",
    "\n",
    "n, feature_size = x.shape\n",
    "names = x.drop([\"treated\"], axis=1).columns\n",
    "treatment_index = x.columns.get_loc(\"treated\")\n",
    "iss_index = x.columns.get_loc(\"iss\")\n",
    "sex_index = x.columns.get_loc(\"sex_M\")\n",
    "\n",
    "var_index = [i for i in range(feature_size) if i != treatment_index and i != iss_index]\n",
    "\n",
    "x_norm = normalize_data(x)\n",
    "\n",
    "## impute missing value\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(x_norm)\n",
    "x_train_scaled = imp.transform(x_norm)\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "                                             x_train_scaled,  \n",
    "                                             fluid_cohort[\"outcome\"], \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=10,\n",
    "                                             stratify=fluid_cohort[\"treated\"]\n",
    "                                    )\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "                                             x_train,  \n",
    "                                             y_train, \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=10,\n",
    "                                             stratify=x_train[:, treatment_index]\n",
    "                                    )\n",
    "\n",
    "\n",
    "w_train = x_train[:, treatment_index]\n",
    "w_val = x_val[:, treatment_index]\n",
    "w_test =  x_test[:, treatment_index]\n",
    "\n",
    "iss_train = x_train[:, iss_index]\n",
    "iss_test =  x_test[:, iss_index]\n",
    "\n",
    "x_train = x_train[:,var_index]\n",
    "x_val = x_val[:, var_index]\n",
    "x_test = x_test[:, var_index]\n",
    "\n",
    "x_learner = pseudo_outcome_nets.XLearner( \n",
    "    \n",
    "    x_train.shape[1],\n",
    "    binary_y=(len(np.unique(y_train)) == 2),\n",
    "    n_layers_out=2,\n",
    "    n_units_out=100,\n",
    "    batch_size=128,\n",
    "    n_iter=1500,\n",
    "    nonlin=\"relu\",\n",
    "    device=\"cuda:1\",\n",
    ")\n",
    "dr_learner = pseudo_outcome_nets.DRLearner( \n",
    "    \n",
    "    x_train.shape[1],\n",
    "    binary_y=(len(np.unique(y_train)) == 2),\n",
    "    n_layers_out=2,\n",
    "    n_units_out=100,\n",
    "    batch_size=128,\n",
    "    n_iter=1500,\n",
    "    nonlin=\"relu\",\n",
    "    device=\"cuda:1\",\n",
    ")\n",
    "\n",
    "\n",
    "x_learner.fit(x_train, y_train, w_train)\n",
    "dr_learner.fit(x_train, y_train, w_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "905b53c7-82e6-4f5a-99db-f571eaed7699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC for propensity model 0.836677367576244\n",
      "25.912621671914522\n",
      "25.889105983982407\n"
     ]
    }
   ],
   "source": [
    "xgb_plugin0, xgb_plugin1, rf, m = train_nuisance_models(x_train, y_train, w_train)\n",
    "\n",
    "mu_0 = xgb_plugin0.predict_proba(x_test)[:, 1]\n",
    "mu_1 = xgb_plugin1.predict_proba(x_test)[:, 1]\n",
    "\n",
    "mu = m.predict_proba(x_test)[:, 1]\n",
    "p = rf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "print(\"AUROC for propensity model\", roc_auc_score(w_test, p))\n",
    "\n",
    "t_plugin = mu_1 - mu_0\n",
    "ident = np.ones(len(p))\n",
    "\n",
    "x_prediction = x_learner.predict(X=x_test).detach().cpu().numpy().flatten()\n",
    "dr_prediction = dr_learner.predict(X=x_test).detach().cpu().numpy().flatten()\n",
    "\n",
    "print(calculate_pseudo_outcome_pehe_dr(w_test, p, x_prediction, y_test, mu_1, mu_0))\n",
    "print(calculate_pseudo_outcome_pehe_dr(w_test, p, dr_prediction, y_test, mu_1, mu_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c35378c9-cefd-4d65-8b8e-83b465a69dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.01433022804545584 0.011617819761209798\n",
      "-0.005490383017108549 0.020817455236621485\n",
      "['edgcs', 'sex_M', 'scenegcs', 'traumatype_B', 'causecode_CUT']\n",
      "['temps2', 'scenefirstbloodpressure', 'NA', 'sex_M', 'HGB']\n",
      "['temps2', 'sex_M', 'NA', 'HGB', 'traumatype_B']\n",
      "mean ISS:  29.641791044776117\n",
      "original -0.0008682435838064082\n",
      "original - iss 33.169642857142854 25.202247191011242\n",
      "===================================\n",
      "642 327\n",
      "shap - auroc 0.46904761904761905\n",
      "shap - ATE -0.00016463708347711409\n",
      "642 327\n",
      "shap - 0  - auroc 0.49627976190476186\n",
      "shap - 0  - ATE -0.0007313821346738351\n",
      "642 327\n",
      "ig - auroc 0.4745535714285714\n",
      "ig - ATE -0.0004775261580415321\n",
      "642 327\n",
      "clinical - auroc 0.4967261904761904\n",
      "clinical - ATE -0.00039490299680607885\n",
      "642 327\n",
      "full features - auroc 0.5513392857142857\n",
      "full features - ATE 1.108139533961416e-05\n",
      "642 327\n",
      "random features - auroc 0.44434523809523807\n",
      "random features - ATE -0.0016957295605397054\n"
     ]
    }
   ],
   "source": [
    "results_train = pickle.load(open(\"../results/massive_trans/train_xlearner.pkl\", \"rb\"))\n",
    "results_test = pickle.load(open(\"../results/massive_trans/test_xlearner.pkl\", \"rb\"))\n",
    "\n",
    "print(np.mean(results_train), np.std(results_train)/np.sqrt(results_train.shape[1]))\n",
    "print(np.mean(results_test), np.std(results_test)/np.sqrt(results_test.shape[1]))\n",
    "\n",
    "estimated_ate_train = np.mean(results_train, axis=0)\n",
    "estimated_ate_test = np.mean(results_test, axis=0)\n",
    "threshold = np.mean(estimated_ate_train)\n",
    "\n",
    "y_true_train = (estimated_ate_train > threshold)\n",
    "y_true_test = (estimated_ate_test > threshold)\n",
    "\n",
    "scenefirstbloodpressure = x.columns.get_loc(\"scenefirstbloodpressure\")\n",
    "lac  = x.columns.get_loc(\"LAC\")\n",
    "inr  = x.columns.get_loc(\"INR\")\n",
    "hgb  = x.columns.get_loc(\"HGB\")\n",
    "\n",
    "explainers = {\n",
    "    \"shap\": feature_idx(\"shap\",\"responder\"), #[gender,trauma ,cause_cut, edgcs, cause_mv ]\n",
    "    \"shap - 0 \": feature_idx(\"shap - 0 \",\"responder\" ), #[temp, ph, bd, hgb, trauma ]\n",
    "    \"ig\": feature_idx(\"ig\",\"responder\" ), #[ph, na, temp, gender, trauma ],\n",
    "    \n",
    "    \"clinical\": [lac, inr, hgb,scenefirstbloodpressure ],\n",
    "    \"full features\": [ i for i in range(x_train.shape[1])],\n",
    "    \"random features\": np.random.randint(x_train.shape[1], size=(5)),\n",
    "}\n",
    "\n",
    "print(\"mean ISS: \", np.mean(iss_test)*74+1)\n",
    "print(\"original\", np.sum(estimated_ate_test[w_test==1])/n)\n",
    "print(\"original - iss\", np.mean(iss_test[w_test==1])*74+1, np.mean(iss_test[w_test==0])*74+1)\n",
    "print(\"===================================\")\n",
    "\n",
    "for explainer, features in explainers.items():\n",
    "    subgroup_ate(\n",
    "        explainer,\n",
    "        features,\n",
    "        y_true_train,\n",
    "        y_true_test,\n",
    "        estimated_ate_test,\n",
    "        iss_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf36707-bff1-459b-a85f-4f4f8e991b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ist3 = pd.read_sas(\"../data/datashare_aug2015.sas7bdat\")\n",
    "\n",
    "continuous_vars = [\n",
    "                    \"gender\",\n",
    "                    \"age\",\n",
    "                    \"weight\",\n",
    "                    \"glucose\",\n",
    "                    \"gcs_eye_rand\",\n",
    "                    \"gcs_motor_rand\",\n",
    "                    \"gcs_verbal_rand\",\n",
    "                    # \"gcs_score_rand\",   \n",
    "                     \"nihss\" ,\n",
    "                     \"sbprand\",\n",
    "                     \"dbprand\",\n",
    "                     \"antiplat_rand\"\n",
    "                  ]\n",
    "\n",
    "cate_variables = [\n",
    "                     # \"livealone_rand\",\n",
    "                     # \"indepinadl_rand\",\n",
    "                     \"infarct\",\n",
    "                     # \"atrialfib_rand\",\n",
    "                    #  \"liftarms_rand\",\n",
    "                    # \"ablewalk_rand\",\n",
    "                    # \"weakface_rand\",\n",
    "                    # \"weakarm_rand\",\n",
    "                    # \"weakleg_rand\",\n",
    "                    # \"dysphasia_rand\",\n",
    "                    # \"hemianopia_rand\",\n",
    "                    # \"visuospat_rand\",\n",
    "                    # \"brainstemsigns_rand\",\n",
    "                    # \"otherdeficit_rand\",\n",
    "                    \"stroketype\"\n",
    "                 ]\n",
    "\n",
    "outcomes = [\"dead7\",\"dead6mo\",\"aliveind6\"]\n",
    "treatment = [\"itt_treat\"]\n",
    "\n",
    "x = ist3[continuous_vars + cate_variables + treatment]\n",
    "\n",
    "x = pd.get_dummies(x, columns=cate_variables)\n",
    "\n",
    "n, feature_size = x.shape\n",
    "\n",
    "\n",
    "names = x.drop([\"itt_treat\"], axis=1).columns\n",
    "treatment_index = x.columns.get_loc(\"itt_treat\")\n",
    "\n",
    "x.iloc[:, treatment_index] = np.where(x[\"antiplat_rand\"]== 2, 0, 1)\n",
    "\n",
    "var_index = [i for i in range(feature_size) if i != treatment_index]\n",
    "\n",
    "x_norm = normalize_data(x)\n",
    "\n",
    "## impute missing value\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(x_norm)\n",
    "x_train_scaled = imp.transform(x_norm)\n",
    "\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(\n",
    "                                             x_train_scaled,  \n",
    "                                             ist3[\"aliveind6\"], \n",
    "                                             test_size=0.2, \n",
    "                                             random_state=20,\n",
    "                                    )\n",
    "\n",
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(\n",
    "                                            x_train,\n",
    "                                            y_train,\n",
    "                                            test_size=0.2,\n",
    "                                            random_state=50,\n",
    "                                    )\n",
    "\n",
    "\n",
    "w_train = x_train[:, treatment_index] == 0\n",
    "w_test =  x_test[:, treatment_index] == 0\n",
    "\n",
    "x_train = x_train[:,var_index]\n",
    "x_test = x_test[:, var_index]\n",
    "\n",
    "y_train = y_train ==0\n",
    "y_test = y_test ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba6dfda-3782-4257-bf5b-6bb749dc839d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0009327315803453758 0.0020149005623565633\n",
      "-0.0004992664351202456 0.003730756822059862\n",
      "['infarct_1.0', 'stroketype_1.0', 'infarct_0.0', 'stroketype_2.0', 'infarct_2.0']\n",
      "['gcs_motor_rand', 'infarct_0.0', 'gcs_eye_rand', 'gcs_verbal_rand', 'gender']\n",
      "['gcs_motor_rand', 'gcs_eye_rand', 'age', 'infarct_0.0', 'stroketype_1.0']\n",
      "shap - auroc 0.4856361818221343\n",
      "shap - ATE -8.246514910741526e-05\n",
      "shap - 0  - auroc 0.4967826786374209\n",
      "shap - 0  - ATE -0.00015270827679673927\n",
      "ig - auroc 0.5237494837068759\n",
      "ig - ATE 3.4763605674863845e-05\n",
      "full features - auroc 0.4898263081237365\n",
      "full features - ATE -0.000190957378167613\n",
      "random features - auroc 0.5113475794004478\n",
      "random features - ATE 5.032881814756329e-05\n"
     ]
    }
   ],
   "source": [
    "results_train = pickle.load(open(\"../results/ist3/train_xlearner.pkl\", \"rb\"))\n",
    "results_test = pickle.load(open(\"../results/ist3/test_xlearner.pkl\", \"rb\"))\n",
    "\n",
    "print(np.mean(results_train), np.std(results_train)/np.sqrt(results_train.shape[1]))\n",
    "print(np.mean(results_test), np.std(results_test)/np.sqrt(results_test.shape[1]))\n",
    "\n",
    "estimated_ate_train = np.mean(results_train, axis=0)\n",
    "estimated_ate_test = np.mean(results_test, axis=0)\n",
    "threshold = np.mean(estimated_ate_train)\n",
    "\n",
    "y_true_train = (estimated_ate_train > threshold)\n",
    "y_true_test = (estimated_ate_test > threshold)\n",
    "\n",
    "# scenefirstbloodpressure = x.columns.get_loc(\"scenefirstbloodpressure\")\n",
    "# lac  = x.columns.get_loc(\"LAC\")\n",
    "# inr  = x.columns.get_loc(\"INR\")\n",
    "# hgb  = x.columns.get_loc(\"HGB\")\n",
    "\n",
    "explainers = {\n",
    "    \"shap\": feature_idx(\"shap\",\"ist3\"), #[gender,trauma ,cause_cut, edgcs, cause_mv ]\n",
    "    \"shap - 0 \": feature_idx(\"shap - 0 \",\"ist3\" ), #[temp, ph, bd, hgb, trauma ]\n",
    "    \"ig\": feature_idx(\"ig\",\"ist3\" ), #[ph, na, temp, gender, trauma ],\n",
    "    \n",
    "    # \"clinical\": [lac, inr, hgb,scenefirstbloodpressure ],\n",
    "    \"full features\": [ i for i in range(x_train.shape[1])],\n",
    "    \"random features\": np.random.randint(x_train.shape[1], size=(5)),\n",
    "}\n",
    "\n",
    "for explainer, features in explainers.items():\n",
    "    subgroup_ate(\n",
    "        explainer,\n",
    "        features,\n",
    "        y_true_train,\n",
    "        y_true_test,\n",
    "        estimated_ate_test,\n",
    "        None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d06d9b-14e5-43ba-a3c4-634511aba502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6., 15., 13., 17.,  8., 15.,  8.,  3.,  1.,  2.]),\n",
       " array([ 1. ,  8.4, 15.8, 23.2, 30.6, 38. , 45.4, 52.8, 60.2, 67.6, 75. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.hist(iss_test[y_pred==1]*74+1)\n",
    "plt.hist(iss_test[y_pred==0]*74+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a7fc6-96c2-4bf9-8d86-e548f10f965b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
